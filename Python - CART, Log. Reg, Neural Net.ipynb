{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a656391-c85c-4fbd-8a6a-1c5537479e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A(i)\n",
      "Full Tree - Training Set Performance:\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 0.0000\n",
      "       Root Mean Squared Error (RMSE) : 0.0000\n",
      "            Mean Absolute Error (MAE) : 0.0000\n",
      "          Mean Percentage Error (MPE) : 0.0000\n",
      "Mean Absolute Percentage Error (MAPE) : 0.0000\n",
      "Full Tree - Validation Set Performance:\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : -19.3078\n",
      "       Root Mean Squared Error (RMSE) : 1500.0279\n",
      "            Mean Absolute Error (MAE) : 1107.2035\n",
      "          Mean Percentage Error (MPE) : -1.1716\n",
      "Mean Absolute Percentage Error (MAPE) : 11.1827\n",
      "-----\n",
      "Part A(ii)\n",
      "Top 4 important features:\n",
      "     Feature  Importance\n",
      "1  Age_08_04    0.665403\n",
      "0         Id    0.139642\n",
      "5         HP    0.048441\n",
      "3   Mfg_Year    0.036904\n",
      "Tree depth: 27\n",
      "Number of leaves: 717\n",
      "-----\n",
      "Part A(iv)\n",
      "Best parameters from GridSearchCV:  {'max_depth': 7, 'min_samples_split': 8}\n",
      "Best Tree - Training Set Performance:\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : -0.0000\n",
      "       Root Mean Squared Error (RMSE) : 787.2831\n",
      "            Mean Absolute Error (MAE) : 599.8986\n",
      "          Mean Percentage Error (MPE) : -0.6527\n",
      "Mean Absolute Percentage Error (MAPE) : 6.0710\n",
      "Best Tree - Validation Set Performance:\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 2.8368\n",
      "       Root Mean Squared Error (RMSE) : 1276.0870\n",
      "            Mean Absolute Error (MAE) : 985.5363\n",
      "          Mean Percentage Error (MPE) : -0.9821\n",
      "Mean Absolute Percentage Error (MAPE) : 9.8529\n",
      "Tree depth: 7\n",
      "Number of leaves: 65\n",
      "-----\n",
      "-----\n",
      "Part B(ii)\n",
      "Classification Tree - Training Set Performance:\n",
      "Confusion Matrix (Accuracy 1.0000)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
      "     0   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     1   0  62   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     2   0   0 179   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     3   0   0   0 251   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     4   0   0   0   0 109   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     5   0   0   0   0   0  83   0   0   0   0   0   0   0   0   0   0   0\n",
      "     6   0   0   0   0   0   0  58   0   0   0   0   0   0   0   0   0   0\n",
      "     7   0   0   0   0   0   0   0  14   0   0   0   0   0   0   0   0   0\n",
      "     8   0   0   0   0   0   0   0   0  32   0   0   0   0   0   0   0   0\n",
      "     9   0   0   0   0   0   0   0   0   0  13   0   0   0   0   0   0   0\n",
      "    10   0   0   0   0   0   0   0   0   0   0  23   0   0   0   0   0   0\n",
      "    11   0   0   0   0   0   0   0   0   0   0   0  13   0   0   0   0   0\n",
      "    12   0   0   0   0   0   0   0   0   0   0   0   0  10   0   0   0   0\n",
      "    13   0   0   0   0   0   0   0   0   0   0   0   0   0   5   0   0   0\n",
      "    14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   0\n",
      "    15   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "Classification Tree - Validation Set Performance:\n",
      "Confusion Matrix (Accuracy 0.4052)\n",
      "\n",
      "       Prediction\n",
      "Actual  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n",
      "     0  0  5  1  1  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "     1  2  6 18  7  2  0  1  0  0  0  0  0  0  0  0  0\n",
      "     2  1 14 47 42  7  1  1  0  0  0  0  0  0  0  0  0\n",
      "     3  0  8 42 81 27  5  1  0  0  0  0  0  0  0  0  0\n",
      "     4  0  1  5 27 26 19  3  0  0  0  0  0  0  0  0  0\n",
      "     5  0  0  0  6 15 31 16  1  0  0  0  0  0  0  0  0\n",
      "     6  0  0  0  0  4  9 16  3  0  0  0  0  0  0  0  0\n",
      "     7  0  0  0  0  0  3  2  1  4  0  0  0  0  0  0  0\n",
      "     8  0  0  0  0  0  0  2  3 13  1  1  0  0  0  0  0\n",
      "     9  0  0  0  0  0  0  0  1  1  4  4  0  0  0  0  0\n",
      "    10  0  0  0  0  0  0  0  3  1  4  3  5  0  0  0  0\n",
      "    11  0  0  0  0  0  0  0  0  0  1  1  3  2  0  0  0\n",
      "    12  0  0  0  0  0  0  0  0  0  0  3  1  1  2  0  0\n",
      "    13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "    14  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "    15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "-----\n",
      "Part B(iii)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Classification Tree - Training Set Performance:\n",
      "Confusion Matrix (Accuracy 0.6272)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
      "     0   1   1   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "     1   0  40  14   8   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     2   1  19 107  52   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     3   0   2  26 187  31   4   1   0   0   0   0   0   0   0   0   0   0\n",
      "     4   0   0   3  31  61  10   4   0   0   0   0   0   0   0   0   0   0\n",
      "     5   0   0   0   5  19  45  14   0   0   0   0   0   0   0   0   0   0\n",
      "     6   0   0   0   1   3  12  42   0   0   0   0   0   0   0   0   0   0\n",
      "     7   0   0   0   0   0   1   7   3   2   0   1   0   0   0   0   0   0\n",
      "     8   0   0   0   0   0   0   1   0  26   0   4   1   0   0   0   0   0\n",
      "     9   0   0   0   0   0   0   0   0   2   5   2   4   0   0   0   0   0\n",
      "    10   0   0   0   0   1   0   0   0   1   0  13   8   0   0   0   0   0\n",
      "    11   0   0   0   0   0   0   0   0   1   1   1  10   0   0   0   0   0\n",
      "    12   0   0   0   0   0   0   0   0   0   0   0  10   0   0   0   0   0\n",
      "    13   0   0   0   0   0   0   0   0   0   0   0   5   0   0   0   0   0\n",
      "    14   0   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0\n",
      "    15   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    16   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "Fine-tuned Classification Tree - Validation Set Performance:\n",
      "Confusion Matrix (Accuracy 0.4226)\n",
      "\n",
      "       Prediction\n",
      "Actual  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14\n",
      "     0  0  5  0  2  0  1  0  0  0  0  0  0  0  0  0\n",
      "     1  2  7 17  9  0  0  1  0  0  0  0  0  0  0  0\n",
      "     2  1 20 37 51  3  1  0  0  0  0  0  0  0  0  0\n",
      "     3  0  9 34 97 19  3  2  0  0  0  0  0  0  0  0\n",
      "     4  0  1  3 31 33 10  3  0  0  0  0  0  0  0  0\n",
      "     5  0  0  0  2 19 30 17  1  0  0  0  0  0  0  0\n",
      "     6  0  0  0  0  5  9 16  2  0  0  0  0  0  0  0\n",
      "     7  0  0  0  0  0  2  3  0  4  0  1  0  0  0  0\n",
      "     8  0  0  0  0  0  0  2  0 15  0  3  0  0  0  0\n",
      "     9  0  0  0  0  0  0  0  0  0  1  5  4  0  0  0\n",
      "    10  0  0  0  0  0  0  0  0  4  3  1  8  0  0  0\n",
      "    11  0  0  0  0  0  0  0  0  0  1  0  6  0  0  0\n",
      "    12  0  0  0  0  0  0  0  0  1  0  0  6  0  0  0\n",
      "    13  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "    14  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "\n",
      "Predicted Price from Regression Tree: 10050.173913043478\n",
      "Predicted Price Bin from Classification Tree: 6\n",
      "Predicted Price Range from Classification Tree: 12795.00 - 14202.50\n"
     ]
    }
   ],
   "source": [
    "#Question 9.3ab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pylab as plt\n",
    "from dmba import plotDecisionTree, classificationSummary, regressionSummary\n",
    "\n",
    "car = pd.read_csv('ToyotaCorolla.csv')\n",
    "\n",
    "categorical_cols = car.select_dtypes(include=['object']).columns\n",
    "categorical_cols = categorical_cols.drop('Price', errors='ignore')\n",
    "\n",
    "car_df = pd.get_dummies(car, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "predictors = [col for col in car_df.columns if col not in ['Price', 'Binned_Price']]\n",
    "outcome = 'Price'\n",
    "\n",
    "car_df = car_df.dropna()\n",
    "\n",
    "train_df, valid_df = train_test_split(car_df, test_size=0.4, random_state=1)\n",
    "\n",
    "print('Part A(i)') #Part A(i)\n",
    "X_train = train_df[predictors]\n",
    "y_train = train_df[outcome]\n",
    "X_valid = valid_df[predictors]\n",
    "y_valid = valid_df[outcome]\n",
    "\n",
    "full_tree = DecisionTreeRegressor(random_state=1)\n",
    "full_tree.fit(X_train, y_train)\n",
    "\n",
    "train_pred = full_tree.predict(X_train)\n",
    "valid_pred = full_tree.predict(X_valid)\n",
    "\n",
    "print(\"Full Tree - Training Set Performance:\")\n",
    "regressionSummary(y_train, train_pred)\n",
    "print(\"Full Tree - Validation Set Performance:\")\n",
    "regressionSummary(y_valid, valid_pred)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part A(ii)') #Part A(ii)\n",
    "train_residuals = y_train - train_pred\n",
    "valid_residuals = y_valid - valid_pred\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([train_residuals, valid_residuals], labels=['Training', 'Validation'])\n",
    "plt.title('Residuals: Training vs. Validation Sets')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Q9_residuals_boxplot.jpg')\n",
    "plt.close()\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': full_tree.feature_importances_\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 4 important features:\")\n",
    "print(importance_df.head(4))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "importance_df_sorted = importance_df.sort_values(by='Importance', ascending=True)\n",
    "plt.barh(importance_df_sorted['Feature'], importance_df_sorted['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Q9_feature_importances.jpg')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plot_tree(full_tree, feature_names=X_train.columns, filled=True, rounded=True, fontsize=10)\n",
    "plt.savefig('Q9_full_decision_tree.jpg', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(f\"Tree depth: {full_tree.get_depth()}\")\n",
    "print(f\"Number of leaves: {full_tree.get_n_leaves()}\")\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part A(iv)') #Part A(iv)\n",
    "param_grid = {\n",
    "    'max_depth': range(2, 10), \n",
    "    'min_samples_split': range(5, 15)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(random_state=1), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters from GridSearchCV: \", grid_search.best_params_)\n",
    "\n",
    "best_tree = grid_search.best_estimator_\n",
    "train_pred_best = best_tree.predict(X_train)\n",
    "valid_pred_best = best_tree.predict(X_valid)\n",
    "\n",
    "print(\"Best Tree - Training Set Performance:\")\n",
    "regressionSummary(y_train, train_pred_best)\n",
    "print(\"Best Tree - Validation Set Performance:\")\n",
    "regressionSummary(y_valid, valid_pred_best)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plot_tree(best_tree, feature_names=X_train.columns, filled=True, rounded=True, fontsize=10)\n",
    "plt.savefig('Q9_best_decision_tree.jpg', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Tree depth: {best_tree.get_depth()}\")\n",
    "print(f\"Number of leaves: {best_tree.get_n_leaves()}\")\n",
    "\n",
    "print(\"-----\")\n",
    "#Part B(i)\n",
    "car_df['Binned_Price'], bin_edges = pd.cut(car_df['Price'], bins=20, labels=False, retbins=True)\n",
    "\n",
    "X_train_clf, X_valid_clf, y_train_binned, y_valid_binned = train_test_split(\n",
    "    car_df[predictors], car_df['Binned_Price'], test_size=0.4, random_state=1\n",
    ")\n",
    "print(\"-----\")\n",
    "print('Part B(ii)') #Part B(ii)\n",
    "classification_tree = DecisionTreeClassifier(random_state=1)\n",
    "classification_tree.fit(X_train_clf, y_train_binned)\n",
    "\n",
    "train_pred_ct = classification_tree.predict(X_train_clf)\n",
    "valid_pred_ct = classification_tree.predict(X_valid_clf)\n",
    "\n",
    "print(\"Classification Tree - Training Set Performance:\")\n",
    "classificationSummary(y_train_binned, train_pred_ct)\n",
    "print(\"Classification Tree - Validation Set Performance:\")\n",
    "classificationSummary(y_valid_binned, valid_pred_ct)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part B(iii)') #Part B(iii)\n",
    "param_grid_clf = {\n",
    "    'max_depth': range(2, 10), \n",
    "    'min_samples_split': range(5, 15)\n",
    "}\n",
    "\n",
    "grid_search_ct = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid_clf, cv=5)\n",
    "grid_search_ct.fit(X_train_clf, y_train_binned)\n",
    "\n",
    "best_ct = grid_search_ct.best_estimator_\n",
    "\n",
    "train_pred_best_ct = best_ct.predict(X_train_clf)\n",
    "valid_pred_best_ct = best_ct.predict(X_valid_clf)\n",
    "\n",
    "print(\"Fine-tuned Classification Tree - Training Set Performance:\")\n",
    "classificationSummary(y_train_binned, train_pred_best_ct)\n",
    "print(\"Fine-tuned Classification Tree - Validation Set Performance:\")\n",
    "classificationSummary(y_valid_binned, valid_pred_best_ct)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plot_tree(best_ct, feature_names=X_train_clf.columns, filled=True, rounded=True, fontsize=10)\n",
    "plt.savefig('Q9_best_classification_tree.jpg', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "car_specifications = pd.DataFrame(columns=predictors)\n",
    "car_specifications.loc[0] = 0\n",
    "\n",
    "car_specifications.loc[0, 'Age_08_04'] = 77\n",
    "car_specifications.loc[0, 'KM'] = 117000\n",
    "car_specifications.loc[0, 'HP'] = 110\n",
    "car_specifications.loc[0, 'Automatic'] = 0\n",
    "car_specifications.loc[0, 'Doors'] = 5\n",
    "car_specifications.loc[0, 'Quarterly_Tax'] = 100\n",
    "car_specifications.loc[0, 'Mfr_Guarantee'] = 0\n",
    "car_specifications.loc[0, 'Guarantee_Period'] = 3\n",
    "car_specifications.loc[0, 'Airco'] = 1\n",
    "car_specifications.loc[0, 'Automatic_airco'] = 0\n",
    "car_specifications.loc[0, 'CD_Player'] = 0\n",
    "car_specifications.loc[0, 'Powered_Windows'] = 0\n",
    "car_specifications.loc[0, 'Sport_Model'] = 0\n",
    "car_specifications.loc[0, 'Tow_Bar'] = 1\n",
    "car_specifications.loc[0, 'Fuel_Type_Petrol'] = 1\n",
    "\n",
    "car_specifications = car_specifications.fillna(0)\n",
    "\n",
    "predicted_price_rt = best_tree.predict(car_specifications)\n",
    "print(\"\\nPredicted Price from Regression Tree:\", predicted_price_rt[0])\n",
    "\n",
    "predicted_bin_ct = best_ct.predict(car_specifications)\n",
    "print(\"Predicted Price Bin from Classification Tree:\", predicted_bin_ct[0])\n",
    "\n",
    "bin_start = bin_edges[int(predicted_bin_ct[0])]\n",
    "bin_end = bin_edges[int(predicted_bin_ct[0]) + 1]\n",
    "print(f\"Predicted Price Range from Classification Tree: {bin_start:.2f} - {bin_end:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018a8ed-6aeb-4126-b8a7-44c893c7504c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1427fbaa-8aaa-4506-99e7-ac87a4974022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A\n",
      "Category Pivot Table:\n",
      "                       Competitive?\n",
      "Category                          \n",
      "Antique/Art/Craft         0.564972\n",
      "Automotive                0.353933\n",
      "Books                     0.500000\n",
      "Business/Industrial       0.666667\n",
      "Clothing/Accessories      0.504202\n",
      "Coins/Stamps              0.297297\n",
      "Collectibles              0.577406\n",
      "Computer                  0.666667\n",
      "Electronics               0.800000\n",
      "EverythingElse            0.235294\n",
      "Health/Beauty             0.171875\n",
      "Home/Garden               0.656863\n",
      "Jewelry                   0.365854\n",
      "Music/Movie/Game          0.602978\n",
      "Photography               0.846154\n",
      "Pottery/Glass             0.350000\n",
      "SportingGoods             0.725806\n",
      "Toys/Hobbies              0.529915\n",
      "\n",
      "Currency Pivot Table:\n",
      "           Competitive?\n",
      "currency              \n",
      "EUR           0.551595\n",
      "GBP           0.687075\n",
      "US            0.519350\n",
      "\n",
      "EndDay Pivot Table:\n",
      "         Competitive?\n",
      "endDay              \n",
      "Fri         0.466899\n",
      "Mon         0.673358\n",
      "Sat         0.427350\n",
      "Sun         0.485207\n",
      "Thu         0.603960\n",
      "Tue         0.532164\n",
      "Wed         0.480000\n",
      "\n",
      "Duration Pivot Table:\n",
      "           Competitive?\n",
      "Duration              \n",
      "1             0.521739\n",
      "3             0.450704\n",
      "5             0.686695\n",
      "7             0.489142\n",
      "10            0.544554\n",
      "-----\n",
      "Part B\n",
      "\n",
      "Classification Summary with All Predictors:\n",
      "Confusion Matrix (Accuracy 0.7833)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 290  72\n",
      "     1  99 328\n",
      "-----\n",
      "Part C\n",
      "\n",
      "Classification Summary without 'ClosePrice':\n",
      "Confusion Matrix (Accuracy 0.6591)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 211 151\n",
      "     1 118 309\n",
      "-----\n",
      "Part D\n",
      "\n",
      "Data types after converting 'bool' to 'int':\n",
      "const                            float64\n",
      "sellerRating                       int64\n",
      "ClosePrice                       float64\n",
      "OpenPrice                        float64\n",
      "Category_Automotive                int64\n",
      "Category_Books                     int64\n",
      "Category_Business/Industrial       int64\n",
      "Category_Clothing/Accessories      int64\n",
      "Category_Coins/Stamps              int64\n",
      "Category_Collectibles              int64\n",
      "Category_Computer                  int64\n",
      "Category_Electronics               int64\n",
      "Category_EverythingElse            int64\n",
      "Category_Health/Beauty             int64\n",
      "Category_Home/Garden               int64\n",
      "Category_Jewelry                   int64\n",
      "Category_Music/Movie/Game          int64\n",
      "Category_Photography               int64\n",
      "Category_Pottery/Glass             int64\n",
      "Category_SportingGoods             int64\n",
      "Category_Toys/Hobbies              int64\n",
      "currency_GBP                       int64\n",
      "currency_US                        int64\n",
      "endDay_Mon                         int64\n",
      "endDay_Sat                         int64\n",
      "endDay_Sun                         int64\n",
      "endDay_Thu                         int64\n",
      "endDay_Tue                         int64\n",
      "endDay_Wed                         int64\n",
      "Duration_3                         int64\n",
      "Duration_5                         int64\n",
      "Duration_7                         int64\n",
      "Duration_10                        int64\n",
      "dtype: object\n",
      "\n",
      "Data type of y_train before conversion: int64\n",
      "Data type of y_train after conversion: int64\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.497134\n",
      "         Iterations 10\n",
      "\n",
      "Closing Price Coefficient: 0.1237, P-value: 1.1363e-21\n",
      "-----\n",
      "Part E\n",
      "\n",
      "Selected Predictors from Stepwise Regression on Training Data:\n",
      "Index(['Category_Coins/Stamps', 'Category_Electronics',\n",
      "       'Category_EverythingElse', 'Category_Health/Beauty', 'endDay_Mon'],\n",
      "      dtype='object')\n",
      "\n",
      "Classification Summary with Selected Features (Training Data):\n",
      "Confusion Matrix (Accuracy 0.5678)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 296  66\n",
      "     1 275 152\n",
      "-----\n",
      "Part F\n",
      "\n",
      "Selected Predictors from Stepwise Regression on Validation Data:\n",
      "Index(['Category_Automotive', 'Category_Coins/Stamps',\n",
      "       'Category_Health/Beauty', 'Category_Jewelry', 'Duration_5'],\n",
      "      dtype='object')\n",
      "-----\n",
      "Part I\n",
      "\n",
      "Classification Summary with L1 Regularization:\n",
      "Confusion Matrix (Accuracy 0.8175)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 319  43\n",
      "     1 101 326\n",
      "\n",
      "Selected Features from L1 Regularization:\n",
      "Index(['sellerRating', 'ClosePrice', 'OpenPrice', 'endDay_Mon', 'Duration_7'], dtype='object')\n",
      "-----\n",
      "Part J\n",
      "\n",
      "Optimal Cutoff Value: 0.5575\n",
      "\n",
      "Classification Summary with Optimal Cutoff:\n",
      "Confusion Matrix (Accuracy 0.7921)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 317  45\n",
      "     1 119 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/431bzhtx6kl9z4pq8zr31h180000gn/T/ipykernel_3311/4217974216.py:163: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lift_df = df.groupby('decile').apply(\n"
     ]
    }
   ],
   "source": [
    "# Question 10.4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_curve\n",
    "import statsmodels.api as sm\n",
    "from dmba import classificationSummary, gainsChart, liftChart\n",
    "from dmba.metric import AIC_score\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "ebay_data = pd.read_csv('eBayAuctions.csv')\n",
    "\n",
    "print('Part A') #Part A\n",
    "category_pivot = ebay_data.pivot_table(values='Competitive?', index='Category', aggfunc='mean')\n",
    "currency_pivot = ebay_data.pivot_table(values='Competitive?', index='currency', aggfunc='mean')\n",
    "endday_pivot = ebay_data.pivot_table(values='Competitive?', index='endDay', aggfunc='mean')\n",
    "duration_pivot = ebay_data.pivot_table(values='Competitive?', index='Duration', aggfunc='mean')\n",
    "\n",
    "print(\"Category Pivot Table:\\n\", category_pivot)\n",
    "print(\"\\nCurrency Pivot Table:\\n\", currency_pivot)\n",
    "print(\"\\nEndDay Pivot Table:\\n\", endday_pivot)\n",
    "print(\"\\nDuration Pivot Table:\\n\", duration_pivot)\n",
    "\n",
    "ebay_data_dummies = pd.get_dummies(ebay_data, columns=['Category', 'currency', 'endDay', 'Duration'], drop_first=True)\n",
    "\n",
    "X = ebay_data_dummies.drop(columns=['Competitive?'])\n",
    "y = ebay_data_dummies['Competitive?']\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part B') #Part B\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = logistic_model.predict_proba(X_valid)[:, 1]\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Summary with All Predictors:\")\n",
    "classificationSummary(y_valid, y_pred)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part C') #Part C\n",
    "X_without_price = X.drop(columns=['ClosePrice'])\n",
    "X_train_no_price, X_valid_no_price, y_train_no_price, y_valid_no_price = train_test_split(\n",
    "    X_without_price, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "logistic_model_no_price = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "logistic_model_no_price.fit(X_train_no_price, y_train_no_price)\n",
    "\n",
    "y_pred_prob_no_price = logistic_model_no_price.predict_proba(X_valid_no_price)[:, 1]\n",
    "y_pred_no_price = (y_pred_prob_no_price >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Summary without 'ClosePrice':\")\n",
    "classificationSummary(y_valid_no_price, y_pred_no_price)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part D') #Part D\n",
    "X_train_with_const = sm.add_constant(X_train)\n",
    "\n",
    "bool_columns = X_train_with_const.select_dtypes(include=['bool']).columns\n",
    "X_train_with_const[bool_columns] = X_train_with_const[bool_columns].astype(int)\n",
    "\n",
    "print(\"\\nData types after converting 'bool' to 'int':\")\n",
    "print(X_train_with_const.dtypes)\n",
    "\n",
    "print(\"\\nData type of y_train before conversion:\", y_train.dtype)\n",
    "if y_train.dtype == 'bool':\n",
    "    y_train = y_train.astype(int)\n",
    "print(\"Data type of y_train after conversion:\", y_train.dtype)\n",
    "\n",
    "logit_model = sm.Logit(y_train, X_train_with_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "closing_price_coef = result.params['ClosePrice']\n",
    "closing_price_pvalue = result.pvalues['ClosePrice']\n",
    "\n",
    "print(f\"\\nClosing Price Coefficient: {closing_price_coef:.4f}, P-value: {closing_price_pvalue:.4e}\")\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part E') #Part E\n",
    "rfe_selector = RFE(estimator=LogisticRegression(max_iter=1000, solver='liblinear'), n_features_to_select=5)\n",
    "rfe_selector.fit(X_train, y_train)\n",
    "selected_features_train = X_train.columns[rfe_selector.support_]\n",
    "\n",
    "print(\"\\nSelected Predictors from Stepwise Regression on Training Data:\")\n",
    "print(selected_features_train)\n",
    "\n",
    "logistic_model_rfe = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "logistic_model_rfe.fit(X_train[selected_features_train], y_train)\n",
    "\n",
    "y_pred_prob_rfe = logistic_model_rfe.predict_proba(X_valid[selected_features_train])[:, 1]\n",
    "y_pred_rfe = (y_pred_prob_rfe >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Summary with Selected Features (Training Data):\")\n",
    "classificationSummary(y_valid, y_pred_rfe)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part F') #Part F\n",
    "rfe_selector_valid = RFE(estimator=LogisticRegression(max_iter=1000, solver='liblinear'), n_features_to_select=5)\n",
    "rfe_selector_valid.fit(X_valid, y_valid)\n",
    "selected_features_valid = X_valid.columns[rfe_selector_valid.support_]\n",
    "print(\"\\nSelected Predictors from Stepwise Regression on Validation Data:\")\n",
    "print(selected_features_valid)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part I') #Part I\n",
    "logistic_l1 = LogisticRegressionCV(\n",
    "    Cs=10, cv=5, penalty='l1', solver='liblinear', max_iter=1000, random_state=42\n",
    ")\n",
    "logistic_l1.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob_l1 = logistic_l1.predict_proba(X_valid)[:, 1]\n",
    "y_pred_l1 = (y_pred_prob_l1 >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Summary with L1 Regularization:\")\n",
    "classificationSummary(y_valid, y_pred_l1)\n",
    "\n",
    "selected_features_l1 = X_train.columns[logistic_l1.coef_[0] != 0]\n",
    "print(\"\\nSelected Features from L1 Regularization:\")\n",
    "print(selected_features_l1)\n",
    "\n",
    "print(\"-----\")\n",
    "print('Part J') #Part J\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, y_pred_prob)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"\\nOptimal Cutoff Value: {optimal_threshold:.4f}\")\n",
    "\n",
    "y_pred_optimal = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "print(\"\\nClassification Summary with Optimal Cutoff:\")\n",
    "classificationSummary(y_valid, y_pred_optimal)\n",
    "\n",
    "#Part K\n",
    "def plot_gains(y_true, y_pred_prob):\n",
    "    df = pd.DataFrame({'y_true': y_true.values, 'y_pred_prob': y_pred_prob})\n",
    "    df = df.sort_values(by='y_pred_prob', ascending=False).reset_index(drop=True)\n",
    "    df['cum_positive'] = df['y_true'].cumsum()\n",
    "    total_positives = df['y_true'].sum()\n",
    "    df['gain'] = df['cum_positive'] / total_positives\n",
    "    df['percentage'] = (df.index + 1) / len(df)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(df['percentage'], df['gain'], label='Model', color='blue')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random', color='grey')\n",
    "    plt.xlabel('Percentage of Sample')\n",
    "    plt.ylabel('Percentage of Positive Responses')\n",
    "    plt.title('Gains Chart')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Q10_Gains_Chart.jpg', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_lift(y_true, y_pred_prob, n_bins=10):\n",
    "    df = pd.DataFrame({'y_true': y_true.values, 'y_pred_prob': y_pred_prob})\n",
    "    df['decile'] = pd.qcut(df['y_pred_prob'], q=n_bins, duplicates='drop', labels=False)\n",
    "    overall_response_rate = df['y_true'].mean()\n",
    "    lift_df = df.groupby('decile').apply(\n",
    "        lambda x: x['y_true'].mean() / overall_response_rate\n",
    "    ).reset_index(name='lift')\n",
    "    lift_df = lift_df.sort_values(by='decile', ascending=False).reset_index(drop=True)\n",
    "    lift_df['decile'] = lift_df.index + 1\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(lift_df['decile'], lift_df['lift'], marker='o', linestyle='-')\n",
    "    plt.xlabel('Deciles (1 = Highest Probability)')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.title('Lift Chart')\n",
    "    plt.xticks(lift_df['decile'])\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Q10_Lift_Chart.jpg', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "plot_gains(y_valid, y_pred_prob)\n",
    "plot_lift(y_valid, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14af4e4-9c68-4a37-b791-7764ed5ff292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eefcf6e8-7441-4d8d-9fbf-30c3f607e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights:\n",
      "[array([[-0.09766229,  0.26219066, -0.6018463 ],\n",
      "       [-0.24500208, -0.45806961, -0.49489149]]), array([[0.12627914],\n",
      "       [0.00429165],\n",
      "       [0.2564571 ]])]\n",
      "\n",
      "Initial Intercepts:\n",
      "[array([-0.39379937, -0.20284179, -0.12227882]), array([-0.14535905])]\n",
      "\n",
      "Predictions: [1 0 1 0 0 0]\n",
      "\n",
      "Probabilities:\n",
      "[[0.49484899 0.50515101]\n",
      " [0.50564764 0.49435236]\n",
      " [0.49447033 0.50552967]\n",
      " [0.50146065 0.49853935]\n",
      " [0.50693581 0.49306419]\n",
      " [0.50529471 0.49470529]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "#Question 11.1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from dmba import classificationSummary\n",
    "\n",
    "data = {\n",
    "    'Years': [4, 18, 1, 3, 15, 6],\n",
    "    'Salary': [43, 65, 53, 95, 88, 112],\n",
    "    'Used Credit': [0, 1, 0, 0, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = df[['Years', 'Salary']]\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = df['Used Credit']\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(3), activation='logistic', \n",
    "    solver='lbfgs', max_iter=1, random_state=1)\n",
    "clf.fit(X_scaled, y)\n",
    "\n",
    "print('Initial Weights:')\n",
    "print(clf.coefs_)\n",
    "print()\n",
    "print('Initial Intercepts:')\n",
    "print(clf.intercepts_)\n",
    "\n",
    "predictions = clf.predict(X_scaled)\n",
    "print('\\nPredictions:', predictions)\n",
    "\n",
    "probabilities = clf.predict_proba(X_scaled)\n",
    "print('\\nProbabilities:')\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e5fca-6565-4618-be21-e7be537ff681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca8ee8-a6a0-4bea-8419-39610e7b3da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
